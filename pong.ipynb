{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "pong.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGqocJk_h6wn",
        "colab_type": "text"
      },
      "source": [
        "# Treinando Pong game utilizando Aprendizado por Refor√ßo\n",
        "\n",
        "O objetivo deste tutorial √© aprender o b√°sico e o necess√°rio da √°rea de Aprendizado por Refor√ßo. Vamos nos basear em um dos maiores cl√°ssicos da hist√≥ria dos video-games: **O jogo Pong**.\n",
        "\n",
        "![Pong](https://media2.giphy.com/media/aTGwuEFyg6d8c/giphy.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJnMNk_Aj0lH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fd9c206-cadc-472d-8aa2-03f2471487f8"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (1.9.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUeECgNdkPjJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "454feb10-c42a-456e-f955-c9f916ae84eb"
      },
      "source": [
        "import os\n",
        "os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = \"hide\"\n",
        "\n",
        "import pygame\n",
        "pygame.init()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSyM45Gdkkuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDfR7s4Ok2Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bar:\n",
        "    def __init__(self, x, y, screen_width, screen_height,\n",
        "                 length=20, width=2, velocity=2, horizontal=True, np_random=np.random):\n",
        "        self.np_random = np_random\n",
        "        self.x = int(x)\n",
        "        self.y = int(y)\n",
        "        self.length = length\n",
        "        self.width = width\n",
        "        self.velocity = velocity\n",
        "        self.horizontal = horizontal\n",
        "        self.screen_width = screen_width\n",
        "        self.screen_height = screen_height\n",
        "        self._direction = 0\n",
        "\n",
        "    def draw(self, screen, color=(255, 255, 255)):  # desenhar em pygame\n",
        "        pygame.draw.rect(screen, color, (\n",
        "                         self.x-self.width/2, self.y-self.length/2, self.width, self.length))\n",
        "\n",
        "    # mode = machine | enemy\n",
        "    # arg  = action  | ball\n",
        "    def move(self, arg,  mode='human'):\n",
        "        if mode == 'machine':\n",
        "            actions = {\n",
        "                0: lambda x: x,\n",
        "                1: lambda x: x + self.velocity,\n",
        "                2: lambda x: x - self.velocity,\n",
        "            }\n",
        "            self.y = actions[arg](self.y)\n",
        "\n",
        "        elif mode == 'enemy':\n",
        "            ball = arg\n",
        "            # Depois de come√ßar a se movimentar, o inimigo demora um tempo\n",
        "            # para verificar novamente a posi√ß√£o da bola\n",
        "            if self._direction != 0:\n",
        "                if self.np_random.random() < .08:\n",
        "                    self._direction = 0\n",
        "            # O inimigo s√≥ consegue \"ver\" a bola quando ela est√° pr√≥xima,\n",
        "            # e tem um tempo de resposta aleat√≥rio\n",
        "            elif ball.x >= self.screen_width*.6 and self.np_random.random() < .85:\n",
        "                self._direction = np.sign(ball.y - self.y)\n",
        "            self.y += self.velocity*self._direction\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f'Invalid mode: {mode}')\n",
        "\n",
        "        self.y = np.clip(self.y, 0, self.screen_height)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH89ezk8k8fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Ball:\n",
        "    def __init__(self, x, y, size, velocity=1, np_random=np.random):\n",
        "        self.np_random = np_random\n",
        "        self.x = int(x)\n",
        "        self.y = int(y)\n",
        "        self.size = size\n",
        "        self.abs_velocity = velocity\n",
        "        self.reset_velocity()\n",
        "\n",
        "    def reset_velocity(self):\n",
        "        v = self.abs_velocity\n",
        "        direction = self.np_random.uniform(np.pi/8, np.pi/3)  # first quadrant\n",
        "        direction *= self.np_random.choice([-1, 1])  # right side\n",
        "        direction += self.np_random.choice([0, np.pi])  # left side\n",
        "        self.velocity = [\n",
        "            v*np.cos(direction), -v*np.sin(direction)]\n",
        "\n",
        "    def move(self):\n",
        "        self.x += self.velocity[0]\n",
        "        self.y += self.velocity[1]\n",
        "\n",
        "    def draw(self, screen, color=(255, 255, 255)):\n",
        "        pygame.draw.rect(screen, color, (\n",
        "            self.x - self.size, self.y - self.size, 2*self.size, 2*self.size))\n",
        "\n",
        "    def bounce(self, wall):\n",
        "        lookup_table = {False: [-1, 1],\n",
        "                        True: [1, -1]}\n",
        "        if abs(self.x - wall.x) < (wall.width/2 + self.size - 1) \\\n",
        "                and abs(self.y - wall.y) < (wall.length/2 + self.size):\n",
        "            self.velocity[0] *= lookup_table[wall.horizontal][0]\n",
        "            self.velocity[1] *= lookup_table[wall.horizontal][1]\n",
        "            return True\n",
        "        return False"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBZQqWAHlDzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PongEnv(gym.Env):\n",
        "    _gym_disable_underscore_compat = True\n",
        "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
        "    reward_range = (-float('inf'), float('inf'))\n",
        "    action_space = gym.spaces.Discrete(3)\n",
        "    observation_space = gym.spaces.Box(\n",
        "        low=-np.float32('inf'), high=np.float32('inf'), shape=(4,))\n",
        "\n",
        "    def __init__(self, height=300, width=400, repeat_actions=3,\n",
        "                 bar_velocity=3, ball_velocity=2,\n",
        "                 num_matches=7, fps=50):\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0]),\n",
        "            high=np.array([width, height, width, height]),\n",
        "            dtype=np.float32)\n",
        "\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.num_matches = num_matches\n",
        "        self.fps = fps\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.repeat_actions = repeat_actions\n",
        "\n",
        "        param_names = ['x', 'y', 'length', 'width', 'velocity', 'horizontal']\n",
        "        w, h, vel = width, height, bar_velocity\n",
        "        bar_parameters = [\n",
        "            # (x,    y,   len, width, vel, horizontal)\n",
        "            (w/27,   h/2, h/3, w/50, vel, False),  # jogador\n",
        "            (w-w/27, h/2, h/3, w/50, vel, False),  # oponente\n",
        "            (w/2,    0,   5,   w,    0,   True),   # teto\n",
        "            (w/2,    h,   5,   w,    0,   True),   # ch√£o\n",
        "            (0,      h/2, h,   5,    0,   False),  # parede esq.\n",
        "            (w,      h/2, h,   5,    0,   False),  # parede dir.\n",
        "        ]\n",
        "        # Obs: as paredes esquerda e direita t√™m prop√≥sito puramente est√©tico\n",
        "\n",
        "        self.bars = []\n",
        "        for bar in bar_parameters:\n",
        "            kwargs = dict(zip(param_names, bar))\n",
        "            self.bars.append(\n",
        "                Bar(screen_width=width, screen_height=height, **kwargs))\n",
        "\n",
        "        self.control_bar = self.bars[0]\n",
        "        self.other_bar = self.bars[1]\n",
        "        self.left_wall = self.bars[4]\n",
        "        self.right_wall = self.bars[5]\n",
        "\n",
        "        self.ball = Ball(x=width/2, y=height/2, size=10,\n",
        "                         velocity=ball_velocity)\n",
        "\n",
        "        self.seed()\n",
        "        self.viewer = None\n",
        "        self.screen = None\n",
        "\n",
        "    def reset_match(self):\n",
        "        self.ball.x, self.ball.y = self.width/2, self.height/2\n",
        "        self.control_bar.y = self.height/2\n",
        "        self.other_bar.y = self.height/2\n",
        "        self.ball.reset_velocity()\n",
        "\n",
        "    def _get_state(self):\n",
        "        return np.array([self.control_bar.x, self.control_bar.y,\n",
        "                         self.ball.x, self.ball.y])\n",
        "\n",
        "    def reset(self):\n",
        "        self.reset_match()\n",
        "        self.done = False\n",
        "        self.score = [0, 0]\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        for _ in range(self.repeat_actions):\n",
        "            reward += self._step(action)\n",
        "            if self.done:\n",
        "                break\n",
        "        return self._get_state(), reward, self.done, {}\n",
        "\n",
        "    def _step(self, action):\n",
        "        if self.done:\n",
        "            return 0\n",
        "\n",
        "        self.control_bar.move(action, mode='machine')\n",
        "        self.other_bar.move(self.ball, mode='enemy')\n",
        "        self.ball.move()\n",
        "\n",
        "        for bar in self.bars:\n",
        "            self.ball.bounce(bar)\n",
        "\n",
        "        if self.ball.bounce(self.left_wall):\n",
        "            player_scored = False\n",
        "            self.score[1] += 1\n",
        "        elif self.ball.bounce(self.right_wall):\n",
        "            player_scored = True\n",
        "            self.score[0] += 1\n",
        "        else:  # no points\n",
        "            return 0\n",
        "\n",
        "        reward = 500\n",
        "        if max(self.score) > self.num_matches / 2:\n",
        "            self.done = True\n",
        "            reward += 2000\n",
        "        self.reset_match()\n",
        "        return reward if player_scored else -reward\n",
        "\n",
        "    def draw(self):\n",
        "        if self.screen is None:\n",
        "            self.screen = pygame.Surface((self.width, self.height))\n",
        "        self.screen.fill((20, 20, 20))\n",
        "        for bar in self.bars:\n",
        "            bar.draw(self.screen)\n",
        "        self.ball.draw(self.screen)\n",
        "\n",
        "    def render(self, mode='human', wait=True):\n",
        "        if wait:\n",
        "            self.clock.tick(self.fps)\n",
        "\n",
        "        self.draw()\n",
        "        img = pygame.surfarray.array3d(self.screen).astype(np.uint8)\n",
        "\n",
        "        # Convers√£o de eixos [y][x][canal] para [x][y][canal]\n",
        "        img = np.transpose(img, [1, 0, 2])\n",
        "\n",
        "        if mode == 'rgb_array':\n",
        "            return img\n",
        "        elif mode == 'human':\n",
        "            if self.viewer is None:\n",
        "                from gym.envs.classic_control import rendering\n",
        "                self.viewer = rendering.SimpleImageViewer(maxwidth=self.width)\n",
        "            self.viewer.imshow(img)\n",
        "            self.viewer.window.set_caption(\n",
        "                f'Pong - {self.score[0]} x {self.score[1]} ({sys.argv[0]})')\n",
        "        else:\n",
        "            return super().render(mode=mode)\n",
        "\n",
        "    def close(self):\n",
        "        if self.viewer:\n",
        "            self.viewer.close()\n",
        "            self.viewer = None\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        self.ball.np_random = self.np_random\n",
        "        for bar in self.bars:\n",
        "            bar.np_random = self.np_random\n",
        "        return [seed]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKMeHwoGlOap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EasyPongEnv(PongEnv):\n",
        "    observation_space = gym.spaces.Box(\n",
        "        low=-np.float32('inf'), high=np.float32('inf'), shape=(2,))\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        low = self.observation_space.low\n",
        "        high = self.observation_space.high\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=np.array([low[0] - high[2], low[1] - high[3]]),\n",
        "            high=np.array([high[0] - low[2], high[1] - low[3]]),\n",
        "            dtype=np.float32)\n",
        "\n",
        "    def _get_state(self):\n",
        "        dx = self.control_bar.x - self.ball.x  # s[0] - s[2]\n",
        "        dy = self.control_bar.y - self.ball.y  # s[1] - s[3]\n",
        "        return np.array([dx, dy])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3R0kVcbnq-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "\n",
        "register(\n",
        "    id='pong-normal-v3',\n",
        "    entry_point='PongEnv',\n",
        "    max_episode_steps=7_500,\n",
        ")\n",
        "\n",
        "register(\n",
        "    id='pong-easy-v3',\n",
        "    entry_point='EasyPongEnv',\n",
        "    max_episode_steps=7_500,\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUFocTy3h6wv",
        "colab_type": "text"
      },
      "source": [
        "## üèì Sobre o Pong\n",
        "\n",
        "Come√ßaremos falando sobre o problema, ou seja, sobre o jogo Pong. Este que foi o primeiro jogo de video-game lucrativo da hist√≥ria, publicado em 1972, constando 48 anos de legado.\n",
        "\n",
        "Pong simula uma partida de t√™nis, existem duas \"raquetes\" e uma bola, e o objetivo de cada uma das raquetes √© n√£o somente evitar que a bola passe por ela, como tamb√©m fazer com que esta passe pela linha que a outra raquete protege, criando assim a premissa que sustenta o interesse pelo jogo. Queremos ent√£o desenvolver um algoritmo capaz de &mdash; sem nenhuma explica√ß√£o adicional &mdash; maximizar as suas recompensas, sendo as a√ß√µes, os estados e as recompensas, todas relativas ao jogo Pong. Teremos no final, portanto, um modelo treinado capaz de bom desempenho dentro do ambiente. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nfMaDGzh6ww",
        "colab_type": "text"
      },
      "source": [
        "## üíª Programando..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6xtUm4Ah6wx",
        "colab_type": "text"
      },
      "source": [
        "### Importando o Gym\n",
        "\n",
        "O **[Gym](https://gym.openai.com/)** √© uma biblioteca desenvolvida pela OpenAI que cont√©m v√°rias implementa√ß√µes prontas de ambientes de Aprendizagem por Refor√ßo. Ela √© muito utilizada quando se quer testar um algoritmo de agente sem ter o trabalho de programar seu pr√≥prio ambiente.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/10624937/42135602-b0335606-7d12-11e8-8689-dd1cf9fa11a9.gif\" alt=\"Exemplos de Ambientes do Gym\" class=\"inline\"/>\n",
        "<figcaption>Exemplo de Ambientes do Gym</figcaption>\n",
        "<br>\n",
        "\n",
        "Para se ter acesso a esses ambientes, basta importar o Gym da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rId3coShh6wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfe3x5lkh6w4",
        "colab_type": "text"
      },
      "source": [
        "### O que √© um Ambiente?\n",
        "\n",
        "Um **Ambiente** de Aprendizagem por Refor√ßo √© um espa√ßo que representa o nosso problema, √© o objeto com o qual o nosso agente deve interagir para cumprir sua fun√ß√£o. Isso significa que o agente toma **a√ß√µes** nesse ambiente, e recebe **recompensas** dele com base na qualidade de sua tomada de decis√µes.\n",
        "\n",
        "Todos os ambientes s√£o dotados de um **espa√ßo de observa√ß√µes**, que √© a forma pela qual o agente recebe informa√ß√µes e deve se basear para a tomada de decis√µes, e um **espa√ßo de a√ß√µes**, que especifica as a√ß√µes poss√≠veis do agente. No xadrez, por exemplo, o espa√ßo de observa√ß√µes seria o conjunto de todas as configura√ß√µes diferentes do tabuleiro, e o espa√ßo de a√ß√µes seria o conjunto de todos os movimentos permitidos.\n",
        "\n",
        "<img src=\"https://www.raspberrypi.org/wp-content/uploads/2016/08/giphy-1-1.gif\" alt=\"Uma A√ß√£o do Xadrez\" class=\"inline\"/>\n",
        "\n",
        "### Como Funciona um Ambiente do Gym?\n",
        "\n",
        "Agora que voc√™ j√° sabe o que √© um ambiente, √© preciso entender como nosso agente interage efetivamente com ele. Todos os ambientes do Gym possuem alguns m√©todos simples para facilitar a comunica√ß√£o com eles:\n",
        "\n",
        "<br>\n",
        "\n",
        "| M√©todo         | Funcionalidade                                        |\n",
        "| :------------- |:----------------------------------------------------- |\n",
        "| `reset()`      | Inicializa o ambiente e recebe a observa√ß√£o inicial   |\n",
        "| `step(acao)`   | Executa uma a√ß√£o e recebe a observa√ß√£o e a recompensa |\n",
        "| `render()`     | Renderiza o ambiente                                  |\n",
        "| `close()`      | Fecha o ambiente                                      |\n",
        "\n",
        "<br>\n",
        "\n",
        "Assim, o c√≥digo para interagir com o ambiente costuma seguir o seguinte modelo:\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "env = gym.make(\"Nome do Ambiente\")                   # Cria o ambiente\n",
        "estado = env.reset()                                 # Inicializa o ambiente\n",
        "done = False                                         # Vari√°vel que diz se acabou\n",
        "\n",
        "while not done:\n",
        "    env.render()                                     # Renderiza o ambiente\n",
        "    acao = random()                                  # Define alguma a√ß√£o\n",
        "    estado, recompensa, done, info = env.step(acao)  # Executa uma a√ß√£o\n",
        "    \n",
        "env.close()                                          # Fecha o ambiente\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCcyrIgnh6w4",
        "colab_type": "text"
      },
      "source": [
        "### Criando um Ambiente\n",
        "\n",
        "Para utilizar um dos ambientes do Gym, n√≥s usamos a fun√ß√£o ```gym.make()```, passando o nome do ambiente desejado como par√¢metro e guardando o valor retornado em uma vari√°vel que chamaramos de ```env```. A lista com todos os ambientes do gym pode ser encontrada [aqui](https://gym.openai.com/envs/#classic_control). Nesse workshop, utilizaremos um ambiente de pong do Grupo Turing, que requer a instala√ß√£o do [Turing Envs](https://github.com/GrupoTuring/turing-envs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpdu9GbKh6w5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "385684d8-1c52-4e2f-ab33-fbc1424a90f1"
      },
      "source": [
        "env = gym.make(\"Pong-v0\")\n",
        "env.seed(0)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 592379725]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJB1kF7Ph6w8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Caracter√≠sticas do Pong\n",
        "\n",
        "Antes de treinar qualquer agente, primeiro √© preciso entender melhor quais as caracter√≠sticas do nosso ambiente.\n",
        "\n",
        "O **Espa√ßo de Observa√ß√£o** do pong (modo f√°cil) √© definido por 2 informa√ß√µes:\n",
        "\n",
        "| Estado    | Informa√ß√£o                            |\n",
        "| :-------- | :------------------------------------ |\n",
        "| 0         | Dist√¢ncia _x_ entre a bola e o agente |\n",
        "| 1         | Dist√¢ncia _y_ entre a bola e o agente |\n",
        "\n",
        "Dessa forma, a cada instante recebemos uma lista da observa√ß√£o com o seguinte formato:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "aPd0fOcxh6w9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "outputId": "44014af6-b1e7-4680-e90a-4fc73b07ce4c"
      },
      "source": [
        "print(env.observation_space.sample())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[142 215 172]\n",
            "  [162 241  86]\n",
            "  [190 155 236]\n",
            "  ...\n",
            "  [ 60 169 230]\n",
            "  [193 100  36]\n",
            "  [ 65 129  65]]\n",
            "\n",
            " [[237 135  15]\n",
            "  [230  50 240]\n",
            "  [ 28 105  91]\n",
            "  ...\n",
            "  [190  77 123]\n",
            "  [105  98  54]\n",
            "  [147 212  99]]\n",
            "\n",
            " [[201 120 217]\n",
            "  [  6 229 169]\n",
            "  [212 105  53]\n",
            "  ...\n",
            "  [  1 115 166]\n",
            "  [ 81   2 184]\n",
            "  [ 33 208 237]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[211 188  15]\n",
            "  [ 12  48 253]\n",
            "  [166  30 237]\n",
            "  ...\n",
            "  [218 220 189]\n",
            "  [144  86 109]\n",
            "  [236  76 212]]\n",
            "\n",
            " [[200  51 169]\n",
            "  [249 116 138]\n",
            "  [128  93 132]\n",
            "  ...\n",
            "  [112 112  41]\n",
            "  [ 22 221 118]\n",
            "  [114  68 198]]\n",
            "\n",
            " [[218  51 198]\n",
            "  [205 254 207]\n",
            "  [ 44 246 253]\n",
            "  ...\n",
            "  [113  39 239]\n",
            "  [ 83 225  84]\n",
            "  [ 92  92  39]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEX76Tgfh6xA",
        "colab_type": "text"
      },
      "source": [
        "J√° o **Espa√ßo de A√ß√£o** √© composto por tr√™s a√ß√µes: mover o jogador para cima, baixo, ou deix√°-lo parado:\n",
        "\n",
        "| A√ß√£o | Significado      |\n",
        "| :--- | :--------------- |\n",
        "| 0    | Ficar parado     |\n",
        "| 1    | Mover para baixo |\n",
        "| 2    | Mover para cima  |\n",
        "\n",
        "Por exemplo, para mover a barra para a cima, fazemos `env.step(2)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "qWyhirU1h6xB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a0faf79-6770-4510-adf9-1900985e5465"
      },
      "source": [
        "print(env.action_space.sample())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioRIeKLGh6xE",
        "colab_type": "text"
      },
      "source": [
        "Por fim, cada vez que tomamos uma a√ß√£o, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
        "\n",
        "| Ocorr√™ncia          | Recompensa |\n",
        "| :------------------ | ---------: |\n",
        "| Ponto do Agente     | $+500$     |\n",
        "| Ponto do Oponente   | $-500$     |\n",
        "| Vit√≥ria do Agente   | $+2000$    |\n",
        "| Vit√≥ria do Oponente | $-2000$    |\n",
        "\n",
        "O primeiro jogador a fazer quatro pontos ganha o jogo. Al√©m disso, as recompensas s√£o cumulativas. Isso significa que se o oponente fizer um ponto _e_ ganhar o jogo, a recompensa √© de $-2500$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5TU5-vXh6xE",
        "colab_type": "text"
      },
      "source": [
        "### ‚úç Gym na pr√°tica\n",
        "\n",
        "Agora que entendemos como o Gym funciona, vamos tentar aplicar esse conhecimento criando uma fun√ß√£o que roda um epis√≥dio de Pong tomando a√ß√µes aleat√≥rias!\n",
        "\n",
        "OBS: Lembrete das fun√ß√µes do Gym\n",
        "\n",
        "| M√©todo                 | Funcionalidade                                          |\n",
        "| :--------------------- |:------------------------------------------------------- |\n",
        "| `reset()`              | Inicializa o ambiente e recebe a observa√ß√£o inicial     |\n",
        "| `step(acao)`           | Executa uma a√ß√£o e recebe a observa√ß√£o e a recompensa   |\n",
        "| `render()`             | Renderiza o ambiente                                    |\n",
        "| `close()`              | Fecha o ambiente                                        |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXGFnV90h6xF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c396067c-1a47-459d-c494-9b589f1c4fe8"
      },
      "source": [
        "!apt-get install python-opengl -y\n",
        "\n",
        "!apt install xvfb -y\n",
        "\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install piglet\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Essa fun√ß√£o deve rodar um episodio de Pong escolhendo a√ß√µes aleat√≥rias\n",
        "def rodar_ambiente():\n",
        "    # Criando o ambiente 'turing_envs:pong-easy-v0'\n",
        "    env = gym.make(\"Pong-v0\")\n",
        "\n",
        "    # Resete o ambiente e receba o primeiro estado\n",
        "    estado = env.reset()\n",
        "    img = plt.imshow(env.render('rgb_array'))\n",
        "    # Inicializando done como false\n",
        "    done = False\n",
        "\n",
        "    # Loop de treino\n",
        "    while not done:\n",
        "        # Escolha uma acao aleatoria\n",
        "        acao = env.action_space.sample()\n",
        "\n",
        "        # Tome essa acao e receba as informacoes do estado seguinte\n",
        "        prox_estado, recompensa, done, info = env.step(acao)\n",
        "\n",
        "        # Renderize o ambiente\n",
        "        img.set_data(env.render('rgb_array'))\n",
        "        #env.render()\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        # Atualizando o estado\n",
        "        estado = prox_estado\n",
        "\n",
        "    # Fechando o ambiente\n",
        "    env.close()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (462 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n",
            "Fetched 784 kB in 1s (709 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 146842 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/dc/d628dcdf0b38b8f230e9c2309bfa370d2e3fb95e9e9c260213d10fde91ac/piglet_templates-1.0.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (19.3.0)\n",
            "Collecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.34.2)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igcbzP2vh6xI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "43bc96aa-65c0-46fa-8375-928040b5f68f"
      },
      "source": [
        "# Testando a fun√ß√£o\n",
        "rodar_ambiente()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tr-Agnvh6xL",
        "colab_type": "text"
      },
      "source": [
        "## üë©‚Äçüíª Algoritmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDRo4fIh6xM",
        "colab_type": "text"
      },
      "source": [
        "Primeiramente, precisaremos utilizar uma biblioteca chamada ***NumPy*** para auxiliar nas computa√ß√µes. Esta √© uma biblioteca do Python capaz de manusear diversas computa√ß√µes matem√°ticas com maestria e ser√° importante futuramente para o nosso trabalho."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "mwmhmQvFh6xM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47ed018d-a73a-472d-eadd-4739f47c537a"
      },
      "source": [
        "import numpy as np # Importando a biblioteca NumPy\n",
        "import gym         # Importando a Biblioteca Gym\n",
        "\n",
        "# Criando o nosso Ambiente: Pong\n",
        "env = gym.make(\"Pong-v0\")\n",
        "\n",
        "# N√∫mero total de a√ß√µes: 3\n",
        "# 0 = parado; 1 = baixo; 2 = cima\n",
        "n_acoes = env.action_space.n\n",
        "\n",
        "print('N√∫mero de a√ß√µes:', n_acoes)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N√∫mero de a√ß√µes: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6_ayOLzh6xP",
        "colab_type": "text"
      },
      "source": [
        "### üî¢ Discretizando o nosso Estado\n",
        "\n",
        "Como comentamos anteriormente, o estado que o nosso agente recebe consiste das dist√¢ncias horizontal e vertical da raquete controlada at√© a bola. Dessa forma, se a nossa tela possuir 800 unidades de largura e 600 unidades de altura, a quantidade total de diferentes estados poss√≠veis seria aproximadamente $3 \\times 800 \\times 600 = 960000$.\n",
        "\n",
        "Como Q-Learning √© um algoritmo que guarda em uma tabela as estimativas do Q de cada a√ß√£o para cada estado, esse gigantesco n√∫mero de estados exigiria n√£o somente guardar como atualizar cada um desses Q. N√£o √© uma situa√ß√£o ideal.\n",
        "\n",
        "Para simplificar (e agilizar) a situa√ß√£o, \"discretizar\" os nossos estados √© razo√°vel e esperado. Faremos com que estados similares o suficiente sejam considerados como iguais e comparilhem das mesmas estimativas, j√° que n√£o faz sentido distinguir o estado (502,234) do estado (515,222)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUHZ90gyh6xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discretiza_estado(estado):\n",
        "    return tuple(np.round(x/10) for x in estado)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuUX0-vYh6xU",
        "colab_type": "text"
      },
      "source": [
        "### üîÄ Escolhendo A√ß√µes\n",
        "\n",
        "Para o processo de de escolha de a√ß√£o, √© necess√°rio lembrar do dilema entre **Explora√ß√£o** e **Explota√ß√£o**. Nosso modelo precisa estabelecer um equil√≠brio entre **explorar o ambiente**, escolhendo a√ß√µes que ele n√£o costuma tomar para encontrar alguma solu√ß√£o que ele n√£o havia pensado antes, e **aproveitar** o conhecimento que j√° possui, tomando a√ß√µes que ele acredita serem as melhores para maximizar as recompensas que receber√° no epis√≥dio.\n",
        "\n",
        "De forma a assegurar que o agente busque tanto novas alternativas que podem gerar melhores resultados quanto seja capaz de utilizar o aprendizado obtido de forma a maximizar seu retorno, existem diversas estrat√©gias para a escolha de explora√ß√£o e explota√ß√£o. Uma das mais utilizadas, que tamb√©m vamos utilizar aqui, √© a sele√ß√£o de a√ß√µes pela estrat√©gia do **\"$\\epsilon$-greedy\"**.\n",
        "\n",
        "#### A Estrat√©gia **$\\epsilon$-greedy**\n",
        "\n",
        "O algoritmo \"$\\epsilon$-greedy\" √© definido da seguinte forma: √© retirado um n√∫mero aleat√≥rio, no intervalo entre 0 e 1. caso este n√∫mero tenha valor inferior ao valor do epsilon, a escolha ser√° de uma a√ß√£o aleat√≥ria, o que configura explora√ß√£o. Caso este n√∫mero seja superior ao epsilon, a a√ß√£o a ser tomada √© a que gera a maior recompensa de acordo com os valores da tabela Q.\n",
        "\n",
        "Este valor de $\\epsilon$ n√£o √© constante ao longo do treinamento. Inicialmente, este valor √© alto, incentivando a maior explora√ß√£o do ambiente. A medida que o treinamento ocorre, mais informa√ß√£o sobre o ambiente √© adquirida, conseguindo uma tabela Q mais representativa da realidade. Dessa forma, quanto mais avan√ßado no treinamento, menor a necessidade de explora√ß√£o e maior a necessidade de exploitar o conhecimento adquirido para maximizar a recompensa. Esta atualiza√ß√£o do $\\epsilon$ √© chamada **\"$\\epsilon$-decay\"** (decaimento do epsilon). Tamb√©m √© estabelecido um valor m√≠nimo para o $\\epsilon$, para que o agente nunca pare completamente de explorar o ambiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMVG-Gwsh6xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constantes da Pol√≠tica Epsilon Greedy\n",
        "# Epsilon: probabilidade de experimentar uma a√ß√£o aleat√≥ria\n",
        "EPSILON = 0.7        # Valor inicial do epsilon\n",
        "EPSILON_MIN = 0.01   # Valor m√≠nimo de epsilon\n",
        "DECAIMENTO = 0.98    # Fator de deca√≠mento do epsilon (por epis√≥dio)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVbKhZnQh6xX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def escolhe_acao(env, Q, estado, epsilon):\n",
        "    # Se n√£o conhecermos ainda o estado, inicializamos o Q de cada a√ß√£o como 0\n",
        "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
        "\n",
        "    # Escolhemos um n√∫mero aleat√≥rio com \"np.random.random()\"\n",
        "    # Se esse n√∫mero for menor que epsilon, tomamos uma a√ß√£o aleat√≥ria\n",
        "    if np.random.random() < epsilon:\n",
        "        # Escolhemos uma a√ß√£o aleat√≥ria, com env.action_space.sample()\n",
        "        acao = env.action_space.sample()\n",
        "    else:\n",
        "        # Escolhemos a melhor a√ß√£o para o estado atual, com np.argmax()\n",
        "        acao = np.argmax(Q[estado])\n",
        "    return acao"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K8iLDOXh6xa",
        "colab_type": "text"
      },
      "source": [
        "Para rodar uma partida, s√£o necess√°rias algumas etapas. Inicialmente, o ambiente √© reiniciado, de forma a inicar um novo epis√≥dio. Em seguida, √© necess√°rio discretizar o estado, pelos motivos j√° explicados acima. Esta discretiza√ß√£o deve ocorrer toda vez em que estamos em um novo estado.\n",
        "\n",
        "Enquanto o ambiente n√£o chega em seu estado terminal, indicado pela vari√°vel \"done\", ser√° feito o processo de escolha de a√ß√µes e, uma vez escolhida, deve-se receber do ambiente o pr√≥ximo estado, a recompensa que a a√ß√£o escolhida gerou, al√©m do sinal se estamos no estado terminal. Todo o processo √© repetido novamente para o pr√≥ximo estado, at√© o final do epis√≥dio.\n",
        "\n",
        "Como explicado na se√ß√£o sobre a biblioteca \"Gym\", \"env.render()\" tem como papel mostrar o ambiente (neste caso, a partida de Pong)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3LTnX7qh6xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def roda_partida(env, Q, renderiza=True):\n",
        "    # Resetamos o ambiente\n",
        "    estado = env.reset()\n",
        "\n",
        "    # Discretizamos o estado\n",
        "    estado = discretiza_estado(estado)\n",
        "    \n",
        "    done = False\n",
        "    retorno = 0\n",
        "    \n",
        "    while not done:\n",
        "        # Escolhemos uma a√ß√£o\n",
        "        acao = escolhe_acao(env, Q, estado, epsilon=0)\n",
        "\n",
        "        # Tomamos nossa a√ß√£o escolhida e recebemos informa√ß√µes do pr√≥ximo estado\n",
        "        prox_estado, recompensa, done, info = env.step(acao)\n",
        "\n",
        "        # Discretizamos o pr√≥ximo estado\n",
        "        prox_estado = discretiza_estado(prox_estado)\n",
        "\n",
        "        # Renderizamos o ambiente\n",
        "        if renderiza:\n",
        "            env.render()\n",
        "\n",
        "        retorno += recompensa\n",
        "        estado = prox_estado\n",
        "\n",
        "    print(f'retorno {retorno:.1f},  '\n",
        "          f'placar {env.score[0]}x{env.score[1]}')\n",
        "    \n",
        "    env.close()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "evZ3MSahh6xd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "4c910a6b-14cf-4f5c-99b5-ab80016c80e8"
      },
      "source": [
        "# Rodamos uma partida de Pong\n",
        "Q = {}\n",
        "roda_partida(env, Q)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-39ce5a3b10df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Rodamos uma partida de Pong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mroda_partida\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-1972c59a16ab>\u001b[0m in \u001b[0;36mroda_partida\u001b[0;34m(env, Q, renderiza)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Escolhemos uma a√ß√£o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0macao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mescolhe_acao\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Tomamos nossa a√ß√£o escolhida e recebemos informa√ß√µes do pr√≥ximo estado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-11641b0819f2>\u001b[0m in \u001b[0;36mescolhe_acao\u001b[0;34m(env, Q, estado, epsilon)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mescolhe_acao\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Se n√£o conhecermos ainda o estado, inicializamos o Q de cada a√ß√£o como 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mestado\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestado\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_acoes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Escolhemos um n√∫mero aleat√≥rio com \"np.random.random()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scjM6w7-h6xg",
        "colab_type": "text"
      },
      "source": [
        "## üèãÔ∏è‚Äç‚ôÄÔ∏è Treinamento\n",
        "\n",
        "Agora sim chegaremos no treinamento propriamente dito. Usando os conceitos vistos na apresenta√ß√£o e nas se√ß√µes anteriores do notebook, podemos definir a fun√ß√£o de treinamento que vai permitir que o agente aprenda a jogar PONG por meio de Q-Learning tabular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys_dwdorh6xg",
        "colab_type": "text"
      },
      "source": [
        "O algoritmo se baseia na atualiza√ß√£o de estimativas dos valores Q para cada par estado-a√ß√£o, de forma a chegar a uma tabela cada vez mais pr√≥xima da realidade do ambiente. Dessa forma, devemos atualizar cada entrada da tabela de acordo com a **equa√ß√£o do Q-Learning**:\n",
        "\n",
        "$$Q*(s,a) \\leftarrow Q*(s,a) + \\alpha \\cdot \\left[r + \\gamma \\cdot \\max_{a'} (Q(s',a')) - Q(s, a)\\right]$$\n",
        "\n",
        "Esta equa√ß√£o corrige o valor do Q(s,a) de acordo com os valores anteriores somados a uma parecela de corre√ß√£o, de forma a minimizar o erro. A recompensa √© representada por r, enquanto os outros par√¢metros est√£o explicados a seguir:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9IU0Arah6xh",
        "colab_type": "text"
      },
      "source": [
        "* `ALFA` ($\\alpha$): algoritmos de aprendizado de m√°quina costumam precisar de uma forma de serem otimizados. Q-learning trabalha em cima de gradientes, uma entidade matem√°tica que indica a dire√ß√£o para maximizar (ou minimizar) uma fun√ß√£o. Dispondo dessa dire√ß√£o, precisamos informar qual deve ser o tamanho do passo a ser dado antes de atualizar a nova \"dire√ß√£o ideal\".\n",
        "\n",
        "* `GAMA` ($\\gamma$): denota o quanto desejamos que nosso algoritmo considere eventos futuros. Se \"$\\gamma = 1$\", nosso algoritmo avaliar√° que a situa√ß√£o futura ser melhor que a atual √© t√£o importante quanto a recompensa da situa√ß√£o atual em si, por outro lado, se \"$\\gamma = 0$\", os eventos futuros n√£o apresentam import√¢ncia alguma para nosso algoritmo. \n",
        "\n",
        "* `Q` √© um dicion√°rio, ou seja, uma estrtura de dados capaz de buscar elementos de forma r√°pida. N√≥s o usaremos para guardar valores relativos √†s estimativas do algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91tcXobFh6xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hiperpar√¢metros do Q-Learning\n",
        "ALFA = 0.05          # Learning rate\n",
        "GAMA = 0.9           # Fator de desconto\n",
        "\n",
        "# Dicion√°rio dos valores de Q\n",
        "# Chaves: estados; valores: qualidade Q atribuida a cada a√ß√£o\n",
        "Q = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dplT63muh6xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def atualiza_q(Q, estado, acao, recompensa, prox_estado):\n",
        "    # para cada estado ainda n√£o descoberto, iniciamos seu valor como nulo\n",
        "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
        "    if prox_estado not in Q.keys(): Q[prox_estado] = [0] * n_acoes\n",
        "\n",
        "    # equa√ß√£o do Q-Learning\n",
        "    Q[estado][acao] = Q[estado][acao] + ALFA*(recompensa + GAMA*np.max(Q[prox_estado]) - Q[estado][acao])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwcTyyw7h6xn",
        "colab_type": "text"
      },
      "source": [
        "Pickle √© uma maneira de salvar dados em um arquivo independente. Dessa forma, podemos gravar os valores da nossa tabela Q em um arquivo pr√≥prio, ficando dispon√≠vel para ser acessada em outro momento. Assim, podemos efetivamente salvar o modelo treinado para ser utilizado posteriormente. Abaixo, j√° est√£o presentes as fun√ß√µes de salvar e de abrir as tabelas com pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogXaZExVh6xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def salva_tabela(Q, nome = 'model.pickle'):\n",
        "    with open(nome, 'wb') as pickle_out:\n",
        "        pickle.dump(Q, pickle_out)\n",
        "\n",
        "def carrega_tabela(nome = 'model.pickle'):\n",
        "    with open(nome, 'rb') as pickle_out:\n",
        "        return pickle.load(pickle_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcDHx4A6h6xq",
        "colab_type": "text"
      },
      "source": [
        "A fun√ß√£o de treinamento tem estrutura semelhante √† fun√ß√£o roda_partida, conforme visto anteriormente. A cada epis√≥dio, o embiente deve ser reiniciado e discretizado, e deve indicar que o epis√≥dio ainda n√£o chegou em sua condi√ß√£o terminal. Devemos tamb√©m zerar o valor da recompensa, pois n√£o devemos utilizar o retorno do epis√≥dio anterior.\n",
        "\n",
        "Enquanto o epis√≥dio n√£o chega no final, o agente deve escolher uma a√ß√£o e tomar a a√ß√£o escolhida. Uma vez tomada a a√ß√£o, o ambiente fornece o pr√≥ximo estado, a recompensa recebida com a escolha, a indica√ß√£o se o estado √© terminal e informa√ß√µes sobre o ambiente.\n",
        "\n",
        "Em seguida, devemos discretizar o pr√≥ximo estado e atualizar os valores de q, o retorno e o estado atual.\n",
        "\n",
        "Por fim, devemos atualizar o valor do epsilon, de acordo com o m√©todo $\\epsilon$-greedy, onde deve ocorrer o decaimento do epsilon, mas seu valor nunca deve ser inferior ao valor m√≠nimo definido.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhv_wLeSh6xq",
        "colab_type": "text"
      },
      "source": [
        "* `N_EPISODIOS` dita quantas vezes o agente dever√° \"reviver\" o ambiente (vit√≥rias e derrotas) antes de acabar seu treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJIs61ieh6xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPISODIOS = 250    # quantidade de epis√≥dios que treinaremos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb28plrch6xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def treina(env, Q):\n",
        "    retornos = []      # retorno de cada epis√≥dio\n",
        "    epsilon = EPSILON\n",
        "\n",
        "    for episodio in range(1, N_EPISODIOS+1):\n",
        "        # resetar o ambiente\n",
        "        estado = env.reset()\n",
        "        \n",
        "        # discretizar o estado inicial\n",
        "        estado = discretiza_estado(estado)\n",
        "        \n",
        "        done = False\n",
        "        retorno = 0\n",
        "        \n",
        "        while not done:\n",
        "            # politica\n",
        "            acao = escolhe_acao(env, Q, estado, epsilon)\n",
        "\n",
        "            # A a√ß√£o √© tomada e os valores novos s√£o coletados\n",
        "            # O novo estado √© salvo numa nova variavel\n",
        "            prox_estado, recompensa, done, info = env.step(acao)\n",
        "            prox_estado = discretiza_estado(prox_estado)\n",
        "\n",
        "            atualiza_q(Q, estado, acao, recompensa, prox_estado)\n",
        "\n",
        "            retorno += recompensa\n",
        "            estado = prox_estado\n",
        "\n",
        "        epsilon = max(DECAIMENTO*epsilon, EPSILON_MIN)\n",
        "        retornos.append(retorno)\n",
        "\n",
        "        if episodio % 10 == 0:\n",
        "            salva_tabela(Q)\n",
        "\n",
        "        print(f'epis√≥dio {episodio},  '\n",
        "              f'retorno {retorno:7.1f},  '\n",
        "              f'retorno m√©dio (√∫ltimos 10 epis√≥dios) {np.mean(retornos[-10:]):7.1f},  '\n",
        "              f'placar {env.score[0]}x{env.score[1]},  '\n",
        "              f'epsilon: {epsilon:.3f}')\n",
        "        \n",
        "    env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "81hsOHcPh6xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "treina(env, Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI4VDQZlh6xy",
        "colab_type": "text"
      },
      "source": [
        "## üèì Testando nosso Agente Treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "TnURZBalh6xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roda_partida(env, Q)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}